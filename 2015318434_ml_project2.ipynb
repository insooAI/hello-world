{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "2015318434_ml_project2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insooAI/hello-world/blob/main/2015318434_ml_project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNK_16PgNOJb"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UleaHaeuLwYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99b7cee9-51dc-497f-b9b5-f64e0a4184cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U70DQkIuKi4H"
      },
      "source": [
        "# train, test data\n",
        "df = pd.read_csv('/content/drive/My Drive/데이터/TRAIN.csv',encoding='UTF-8')[['Post','Label']]  \n",
        "tdf = pd.read_csv('/content/drive/My Drive/데이터/TEST.csv',encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Ils5DTd2w"
      },
      "source": [
        "tdf[\"Label\"] = 'Unknown'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvR7q0x4Ocrb"
      },
      "source": [
        "df = pd.concat([df,tdf],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbUAEdePQy7"
      },
      "source": [
        "posts = df[\"Post\"].values.tolist()\n",
        "labels = df[\"Label\"].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc7mqQZTP30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "e80c276c-d479-4143-fa8c-51693c5a89a4"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re                                                                                                                                                                                                                                                                                                                                                                                     \n",
        "                                                                                                                                                                                                                                                                                                                                                                                              \n",
        "def cleanHtml(raw_html):                                                                                      \n",
        "    cleanr = re.compile('<.*?>')                                                                              \n",
        "    cleantext = re.sub(cleanr, '', raw_html)                                                                  \n",
        "    return cleantext                                                                                          \n",
        "                                                                                                              \n",
        "def etcRemove(x) :                                                                                         \n",
        "    return re.sub('[^0-9a-zA-Zㄱ-ㅎ가-힣${3}#{3}| |,|.|!|?|`|\\']', \"\",x)                                               \n",
        "                                                                                                                                                                                                                                                                                                                                                                                              \n",
        "def removeURL(x) :                                                                                                                                                                                                                                                                                                                                                                            \n",
        "    return re.sub(\"(https|http|ftp|telnet|news|mms)?://(\\w*:\\w*@)?[-\\w.]+(:\\d+)?(/([\\w/_.]*(\\?\\S+)?)?)?\",\"\",x)\n",
        "\n",
        "def tolower(x) :                                                                                                                                                                                                                                                                                                                                                                              \n",
        "    try :                                                                                                                                                                                                                                                                                                                                                                                     \n",
        "        return x.lower()                                                                                                                                                                                                                                                                                                                                                                      \n",
        "    except :                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "        return \"\" \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def prep(x) :\n",
        "    \n",
        "    return tolower(etcRemove(cleanHtml(removeURL(x))))\n",
        "\n",
        "def remove_stopwords(x): \n",
        "    stops= set(stopwords.words('english'))\n",
        "\n",
        "    mystops = [\"shed\",\"dont\",\"didnt\",\"youre\",\"im\",\"its\",\"wentare\",\"shes\",\"hes\"]\n",
        "\n",
        "    stops = list(stops) + mystops\n",
        "\n",
        "    no_stops = [word for word in x if not word in stops]\n",
        "    return list(no_stops)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPhcyn9NXW5U"
      },
      "source": [
        "posts = list( prep(x) for x in posts ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hwTmmd92-XY"
      },
      "source": [
        "def tokenize(line) :\n",
        "    return nltk.word_tokenize(line)\n",
        "#단어별 토큰화\n",
        "\n",
        "def extract(lines) :\n",
        "    tokens = list( nltk.pos_tag(tokenize(line)) for line in lines)  \n",
        "    return list( list( (x[0],x[1]) for x in y if x[1][:2] == \"NN\" or x[1][:2] == \"VB\" ) for y in tokens )\n",
        "# 동사와 명사만 추출"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU7U7kJv25GJ"
      },
      "source": [
        "tokens = extract(posts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuFMzkL7Cz1d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = list( list(lemmatizer.lemmatize(word,'v') for word,tag in x) for x in tokens)\n",
        "# 원형 복원"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hZXGSbLNFXK"
      },
      "source": [
        "tokens = list(remove_stopwords(x) for x in tokens )\n",
        "#불용어 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R-6mxSlsu56"
      },
      "source": [
        "def count_words(posts) :\n",
        "    d = {} \n",
        "\n",
        "    for p in posts : \n",
        "        for w in p :\n",
        "            try :\n",
        "                d[w] += 1   \n",
        "            except :\n",
        "                d[w] = 1\n",
        "\n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARJzi8dLkEvc"
      },
      "source": [
        "word_count = count_words(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqHg_E83shJx"
      },
      "source": [
        "tokens = list( list( x for x in y if word_count[x] > 1) for y in tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFJVv5WXfs4F"
      },
      "source": [
        "word_count = count_words(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-VVDSYXt8Id",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b53459f-9d48-406b-96a8-58f6c6e53227"
      },
      "source": [
        "sorted(word_count.items(), key=lambda x: x[1], reverse=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"'wanting\", 2),\n",
              " ('kitty', 2),\n",
              " ('better.you', 2),\n",
              " ('ideations', 2),\n",
              " ('horrible', 2),\n",
              " ('hawaii', 2),\n",
              " ('errands', 2),\n",
              " (\"'my\", 2),\n",
              " ('powerless', 2),\n",
              " ('eerie', 2),\n",
              " ('revolution', 2),\n",
              " ('outpouring', 2),\n",
              " ('timeframe', 2),\n",
              " ('clearer', 2),\n",
              " ('times.im', 2),\n",
              " ('salvation', 2),\n",
              " ('amusement', 2),\n",
              " ('intoxicate', 2),\n",
              " ('paul', 2),\n",
              " ('struggeling', 2),\n",
              " ('suicde', 2),\n",
              " ('again..', 2),\n",
              " ('sweetheart', 2),\n",
              " ('gentler', 2),\n",
              " ('underlie', 2),\n",
              " ('cable', 2),\n",
              " ('gently', 2),\n",
              " ('allergies', 2),\n",
              " ('wheres', 2),\n",
              " ('bell', 2),\n",
              " ('panel', 2),\n",
              " ('dork', 2),\n",
              " ('zap', 2),\n",
              " ('extension', 2),\n",
              " ('that.im', 2),\n",
              " ('can.a', 2),\n",
              " (\"'gti\", 2),\n",
              " ('meaninglessness', 2),\n",
              " ('life.if', 2),\n",
              " ('consideration', 2),\n",
              " ('sicker', 2),\n",
              " ('dissonance', 2),\n",
              " ('you.do', 2),\n",
              " ('selfcare', 2),\n",
              " ('youxe2x80x99ll', 2),\n",
              " ('council', 2),\n",
              " ('gollum', 2),\n",
              " ('birthgiver', 2),\n",
              " ('prognosis', 2),\n",
              " ('gaslighting', 2),\n",
              " ('landlords', 2),\n",
              " ('scam', 2),\n",
              " ('cult', 2),\n",
              " ('promotions', 2),\n",
              " ('complex', 2),\n",
              " ('grandparent', 2),\n",
              " ('alcoholic', 2),\n",
              " ('hail', 2),\n",
              " (\"'might\", 2),\n",
              " ('colossal', 2),\n",
              " ('horrific', 2),\n",
              " ('lovingkindness', 2),\n",
              " ('so.i', 2),\n",
              " ('relentless', 2),\n",
              " ('dwells.xe2x80x9dmarcus', 2),\n",
              " ('cptsd', 2),\n",
              " ('complaints', 2),\n",
              " ('exbf', 2),\n",
              " ('gtmy', 2),\n",
              " ('with.im', 2),\n",
              " ('rabbit', 2),\n",
              " ('domicile', 2),\n",
              " ('times.i', 2),\n",
              " ('infact', 2),\n",
              " ('martyr', 2),\n",
              " ('pepsi', 2),\n",
              " ('likewise', 2),\n",
              " ('caleb', 2),\n",
              " ('yourself.i', 2),\n",
              " ('manipulation', 2),\n",
              " ('ape', 2),\n",
              " ('harden', 2),\n",
              " ('anymore.so', 2),\n",
              " ('trance', 2),\n",
              " ('pirate', 2),\n",
              " ('aspergers', 2),\n",
              " ('classroom', 2),\n",
              " ('supermodels', 2),\n",
              " ('mistress', 2),\n",
              " ('life.edit', 2),\n",
              " ('sucker', 2),\n",
              " ('def', 2),\n",
              " ('too.also', 2),\n",
              " ('overestimate', 2),\n",
              " ('older', 2),\n",
              " ('harness', 2),\n",
              " ('delt', 2),\n",
              " ('valleys', 2),\n",
              " ('revel', 2),\n",
              " ('innings', 2),\n",
              " ('wager', 2),\n",
              " ('recede', 2),\n",
              " ('morality', 2),\n",
              " ('prop', 2),\n",
              " ('hatch', 2),\n",
              " ('loyalty', 2),\n",
              " ('eloquent', 2),\n",
              " ('graveyard', 2),\n",
              " ('interviewer', 2),\n",
              " ('inevitability', 2),\n",
              " ('have.you', 2),\n",
              " ('prime', 2),\n",
              " (\"'anytime\", 2),\n",
              " ('hint', 2),\n",
              " ('estrange', 2),\n",
              " (\"'wouldnt\", 2),\n",
              " ('sclerosis', 2),\n",
              " ('bull', 2),\n",
              " ('hearted', 2),\n",
              " ('partnership', 2),\n",
              " (\"'working\", 2),\n",
              " ('exes', 2),\n",
              " ('say.first', 2),\n",
              " ('jar', 2),\n",
              " ('institutionalize', 2),\n",
              " ('exhusband', 2),\n",
              " ('rbipolarreddit', 2),\n",
              " ('outlive', 2),\n",
              " ('creator', 2),\n",
              " ('glimmer', 2),\n",
              " ('fucker', 2),\n",
              " (\"'check\", 2),\n",
              " (\"'would\", 2),\n",
              " ('jebus', 2),\n",
              " ('aloe', 2),\n",
              " ('machinery', 2),\n",
              " (\"'doing\", 2),\n",
              " ('hehe', 2),\n",
              " ('comb', 2),\n",
              " ('rsuicidology', 2),\n",
              " (\"'lots\", 2),\n",
              " ('girl.i', 2),\n",
              " ('evaluation', 2),\n",
              " ('smash', 2),\n",
              " ('ftm', 2),\n",
              " (\"'seconded\", 2),\n",
              " ('prostitute', 2),\n",
              " ('dif', 2),\n",
              " (\"'rraisedbynarcissists\", 2),\n",
              " ('it.that', 2),\n",
              " ('cookies', 2),\n",
              " ('americans', 2),\n",
              " (\"'tldr\", 2),\n",
              " ('scumbag', 2),\n",
              " ('northeast', 2),\n",
              " ('councelor', 2),\n",
              " ('orientation', 2),\n",
              " ('rockbottom', 2),\n",
              " ('ads', 2),\n",
              " ('miserable', 2),\n",
              " ('blackmail', 2),\n",
              " ('criterion', 2),\n",
              " ('reel', 2),\n",
              " ('hypothesis', 2),\n",
              " ('disclaimer', 2),\n",
              " ('allowance', 2),\n",
              " ('exboyfriend', 2),\n",
              " ('routines', 2),\n",
              " ('appearances', 2),\n",
              " ('flood', 2),\n",
              " ('freeze', 2),\n",
              " ('help.when', 2),\n",
              " ('vain', 2),\n",
              " ('obstacle', 2),\n",
              " (\"'knowing\", 2),\n",
              " ('soak', 2),\n",
              " ('thinkim', 2),\n",
              " ('liftin', 2),\n",
              " ('jaw', 2),\n",
              " ('arguin', 2),\n",
              " ('meand', 2),\n",
              " ('marilyn', 2),\n",
              " ('happenin', 2),\n",
              " ('pigeonhole', 2),\n",
              " ('accent', 2),\n",
              " ('capitulate', 2),\n",
              " ('bash', 2),\n",
              " ('steak', 2),\n",
              " ('flavor', 2),\n",
              " ('nose', 2),\n",
              " ('clever', 2),\n",
              " ('distortions', 2),\n",
              " ('absurdity', 2),\n",
              " ('highs', 2),\n",
              " ('variation', 2),\n",
              " ('sole', 2),\n",
              " ('bland', 2),\n",
              " ('again.i', 2),\n",
              " ('whiny', 2),\n",
              " ('fail.my', 2),\n",
              " ('dominate', 2),\n",
              " ('harassment', 2),\n",
              " ('angina', 2),\n",
              " ('biodiversity', 2),\n",
              " ('graph', 2),\n",
              " ('tonne', 2),\n",
              " ('rashes', 2),\n",
              " ('practise', 2),\n",
              " ('hunger', 2),\n",
              " ('max', 2),\n",
              " ('geography', 2),\n",
              " ('avoidance', 2),\n",
              " ('shakespeare', 2),\n",
              " ('thief', 2),\n",
              " (\"'ok\", 2),\n",
              " ('fmla', 2),\n",
              " ('asleep', 2),\n",
              " ('deseril', 2),\n",
              " ('but..', 2),\n",
              " ('hiphop', 2),\n",
              " ('cunninlynguists', 2),\n",
              " ('cudi', 2),\n",
              " ('krizz', 2),\n",
              " ('n9ne', 2),\n",
              " ('sadistik', 2),\n",
              " ('hip', 2),\n",
              " ('dis', 2),\n",
              " ('pullo', 2),\n",
              " ('caesar', 2),\n",
              " ('engulf', 2),\n",
              " ('.38', 2),\n",
              " ('murderer', 2),\n",
              " ('cub', 2),\n",
              " ('reform', 2),\n",
              " ('bumpy', 2),\n",
              " ('vilify', 2),\n",
              " ('anchor', 2),\n",
              " ('virtue', 2),\n",
              " ('pummel', 2),\n",
              " ('dune', 2),\n",
              " ('trivialize', 2),\n",
              " ('thinner', 2),\n",
              " ('thicker', 2),\n",
              " ('hammer', 2),\n",
              " ('prioritize', 2),\n",
              " ('jeopardize', 2),\n",
              " ('booster', 2),\n",
              " ('michael', 2),\n",
              " ('airplane', 2),\n",
              " ('engines', 2),\n",
              " ('yay', 2),\n",
              " ('people.i', 2),\n",
              " ('runners', 2),\n",
              " ('inventory', 2),\n",
              " ('iq', 2),\n",
              " ('squat', 2),\n",
              " ('biceps', 2),\n",
              " ('greener', 2),\n",
              " ('crapper', 2),\n",
              " ('tiger', 2),\n",
              " ('adonis', 2),\n",
              " ('darn', 2),\n",
              " ('scrutiny', 2),\n",
              " ('guygirl', 2),\n",
              " ('petty', 2),\n",
              " ('bros', 2),\n",
              " ('convo', 2),\n",
              " ('it.if', 2),\n",
              " ('footsteps', 2),\n",
              " ('ripple', 2),\n",
              " ('deduce', 2),\n",
              " ('defect', 2),\n",
              " ('vitamin', 2),\n",
              " ('fancy', 2),\n",
              " ('incredibly', 2),\n",
              " ('dede', 2),\n",
              " ('deer', 2),\n",
              " ('heroes', 2),\n",
              " ('representation', 2),\n",
              " ('roam', 2),\n",
              " ('passport', 2),\n",
              " ('production', 2),\n",
              " ('feb', 2),\n",
              " ('accusations', 2),\n",
              " ('recession', 2),\n",
              " ('ireland', 2),\n",
              " ('pub', 2),\n",
              " ('contributors', 2),\n",
              " ('upvotes', 2),\n",
              " ('tint', 2),\n",
              " ('depression.i', 2),\n",
              " ('explanations', 2),\n",
              " ('thirty', 2),\n",
              " ('parade', 2),\n",
              " ('lease', 2),\n",
              " ('assistant', 2),\n",
              " ('reservation', 2),\n",
              " ('politics', 2),\n",
              " ('demeanor', 2),\n",
              " ('becuase', 2),\n",
              " ('prowess', 2),\n",
              " ('exhibit', 2),\n",
              " ('kg', 2),\n",
              " ('metaphysics', 2),\n",
              " ('pancreas', 2),\n",
              " ('grammar', 2),\n",
              " ('pming', 2),\n",
              " ('sleeves', 2),\n",
              " ('cab', 2),\n",
              " ('overtake', 2),\n",
              " ('balloon', 2),\n",
              " ('turmoils', 2),\n",
              " ('strand', 2),\n",
              " ('onto', 2),\n",
              " ('jose', 2),\n",
              " ('diana', 2),\n",
              " ('verdant', 2),\n",
              " ('arteries', 2),\n",
              " ('blush', 2),\n",
              " ('bitter', 2),\n",
              " ('heartfelt', 2),\n",
              " ('concrete', 2),\n",
              " ('gaze', 2),\n",
              " ('betcha', 2),\n",
              " ('trials', 2),\n",
              " ('cunt', 2),\n",
              " ('screenplay', 2),\n",
              " ('magnify', 2),\n",
              " ('manufacture', 2),\n",
              " ('generate', 2),\n",
              " ('ama', 2),\n",
              " ('twenty', 2),\n",
              " ('endall', 2),\n",
              " ('dammit', 2),\n",
              " ('me.dont', 2),\n",
              " ('reputation', 2),\n",
              " ('glaze', 2),\n",
              " ('thursdays', 2),\n",
              " ('humble', 2),\n",
              " ('cheap', 2),\n",
              " ('cosplay', 2),\n",
              " ('gossip', 2),\n",
              " ('marker', 2),\n",
              " ('violation', 2),\n",
              " ('intercourse', 2),\n",
              " ('cynicism', 2),\n",
              " ('gratitude', 2),\n",
              " ('unborn', 2),\n",
              " ('ohio', 2),\n",
              " ('affair', 2),\n",
              " ('organ', 2),\n",
              " ('measles', 2),\n",
              " ('healer', 2),\n",
              " ('bruise', 2),\n",
              " ('fuckers', 2),\n",
              " ('sideline', 2),\n",
              " ('spain', 2),\n",
              " ('prejudice', 2),\n",
              " ('illuminate', 2),\n",
              " ('mathematics', 2),\n",
              " ('density', 2),\n",
              " ('awe', 2),\n",
              " ('slob', 2),\n",
              " ('sweatshirt', 2),\n",
              " ('sewage', 2),\n",
              " ('arewere', 2),\n",
              " ('recipes', 2),\n",
              " ('eyeball', 2),\n",
              " ('rainn', 2),\n",
              " ('mobile', 2),\n",
              " ('abuser', 2),\n",
              " ('warcraft', 2),\n",
              " ('monitor', 2),\n",
              " ('minecraft', 2),\n",
              " ('exs', 2),\n",
              " ('overthinking', 2),\n",
              " ('aps', 2),\n",
              " ('curriculars', 2),\n",
              " ('fuckton', 2),\n",
              " ('seventeen', 2),\n",
              " ('help.and', 2),\n",
              " ('bloody', 2),\n",
              " ('rsex', 2),\n",
              " ('rrelationshipadvice', 2),\n",
              " ('acknowledgement', 2),\n",
              " ('soldier', 2),\n",
              " ('misdemeanors', 2),\n",
              " ('toothbrush', 2),\n",
              " ('atonement', 2),\n",
              " ('tabs', 2),\n",
              " ('policies', 2),\n",
              " ('dissuade', 2),\n",
              " ('ruffle', 2),\n",
              " ('sniper', 2),\n",
              " ('verse', 2),\n",
              " ('know.take', 2),\n",
              " ('abortion', 2),\n",
              " (\"'hell\", 2),\n",
              " ('lamictal', 2),\n",
              " ('commission', 2),\n",
              " ('repayments', 2),\n",
              " ('wive', 2),\n",
              " ('saturday', 2),\n",
              " ('meetup', 2),\n",
              " ('dial', 2),\n",
              " ('borderlands', 2),\n",
              " ('taller', 2),\n",
              " ('constrain', 2),\n",
              " ('somehting', 2),\n",
              " ('totem', 2),\n",
              " ('bound', 2),\n",
              " ('poets', 2),\n",
              " ('replicate', 2),\n",
              " ('fond', 2),\n",
              " ('flair', 2),\n",
              " ('sophomore', 2),\n",
              " ('whiskey', 2),\n",
              " ('soon.this', 2),\n",
              " ('supportive', 2),\n",
              " ('envision', 2),\n",
              " (\"'isnt\", 2),\n",
              " ('finality', 2),\n",
              " ('tenant', 2),\n",
              " ('conjunction', 2),\n",
              " ('supersede', 2),\n",
              " ('chaos', 2),\n",
              " ('numb', 2),\n",
              " ('planner', 2),\n",
              " ('snack', 2),\n",
              " ('wavelength', 2),\n",
              " ('insides', 2),\n",
              " ('pact', 2),\n",
              " ('repetition', 2),\n",
              " ('frat', 2),\n",
              " ('familiarity', 2),\n",
              " ('cocoon', 2),\n",
              " ('seroquil', 2),\n",
              " (\"'they\", 2),\n",
              " ('spray', 2),\n",
              " ('selflove', 2),\n",
              " ('regroup', 2),\n",
              " ('rinse', 2),\n",
              " ('perk', 2),\n",
              " (\"'diagnoses\", 2),\n",
              " ('trend', 2),\n",
              " ('quack', 2),\n",
              " ('destruct', 2),\n",
              " ('poet', 2),\n",
              " ('theraphy', 2),\n",
              " ('wasam', 2),\n",
              " ('vanish', 2),\n",
              " ('heavy', 2),\n",
              " ('polio', 2),\n",
              " ('you.all', 2),\n",
              " ('bulk', 2),\n",
              " ('me.but', 2),\n",
              " ('tolimerence.html', 2),\n",
              " ('urgency', 2),\n",
              " ('tad', 2),\n",
              " ('venue', 2),\n",
              " (\"'then\", 2),\n",
              " ('slack', 2),\n",
              " ('sciences', 2),\n",
              " ('repulse', 2),\n",
              " ('intake', 2),\n",
              " ('calculate', 2),\n",
              " ('overdo', 2),\n",
              " ('minister', 2),\n",
              " ('supervisor', 2),\n",
              " ('george', 2),\n",
              " ('chop', 2),\n",
              " ('me.if', 2),\n",
              " ('chew', 2),\n",
              " ('gum', 2),\n",
              " ('rrdo', 2),\n",
              " ('beverage', 2),\n",
              " ('dosages', 2),\n",
              " ('plight', 2),\n",
              " ('counteract', 2),\n",
              " ('gps', 2),\n",
              " ('sociability', 2),\n",
              " ('generalize', 2),\n",
              " ('bandwagon', 2),\n",
              " ('heartwrenching', 2),\n",
              " ('respectlove', 2),\n",
              " ('outline', 2),\n",
              " ('fortune', 2),\n",
              " ('ignorant', 2),\n",
              " ('condone', 2),\n",
              " ('careerinternship', 2),\n",
              " ('metabolism', 2),\n",
              " ('mike', 2),\n",
              " ('flicker', 2),\n",
              " ('tiptoe', 2),\n",
              " ('hungry', 2),\n",
              " ('time.the', 2),\n",
              " ('treadmill', 2),\n",
              " ('job.i', 2),\n",
              " ('mumble', 2),\n",
              " ('slowly', 2),\n",
              " ('hardship', 2),\n",
              " ('impose', 2),\n",
              " ('vancouver', 2),\n",
              " ('elate', 2),\n",
              " ('rugby', 2),\n",
              " ('intrude', 2),\n",
              " ('lighten', 2),\n",
              " ('do.it', 2),\n",
              " ('problems.im', 2),\n",
              " ('logistics', 2),\n",
              " ('distribution', 2),\n",
              " ('gist', 2),\n",
              " (\"'leave\", 2),\n",
              " ('academy', 2),\n",
              " ('winner', 2),\n",
              " ('balcony', 2),\n",
              " ('developer', 2),\n",
              " ('nonexistence', 2),\n",
              " ('hitler', 2),\n",
              " ('plumb', 2),\n",
              " ('rout', 2),\n",
              " ('bloom', 2),\n",
              " ('thoughts.i', 2),\n",
              " ('disturb', 2),\n",
              " ('phenomenon', 2),\n",
              " ('construe', 2),\n",
              " ('banquet', 2),\n",
              " ('crumb', 2),\n",
              " ('injustice', 2),\n",
              " ('undervalue', 2),\n",
              " ('digress', 2),\n",
              " ('elegy', 2),\n",
              " ('suicide.edit', 2),\n",
              " ('meander', 2),\n",
              " ('dissapointed', 2),\n",
              " ('boot', 2),\n",
              " ('virtues', 2),\n",
              " ('podium', 2),\n",
              " ('tarnish', 2),\n",
              " ('scorn', 2),\n",
              " ('diversity', 2),\n",
              " ('minorities', 2),\n",
              " ('enrol', 2),\n",
              " ('imbalances', 2),\n",
              " ('gardener', 2),\n",
              " ('bush', 2),\n",
              " ('accrue', 2),\n",
              " ('erection', 2),\n",
              " ('circulation', 2),\n",
              " ('leak', 2),\n",
              " ('cancers', 2),\n",
              " ('temp', 2),\n",
              " ('preexist', 2),\n",
              " ('think.im', 2),\n",
              " ('hour..', 2),\n",
              " ('cape', 2),\n",
              " ('selfcontradicting', 2),\n",
              " ('velocity', 2),\n",
              " ('do.if', 2),\n",
              " ('peddle', 2),\n",
              " ('materialize', 2),\n",
              " ('prototype', 2),\n",
              " ('world.please', 2),\n",
              " ('npd', 2),\n",
              " ('selfaware', 2),\n",
              " ('parasite', 2),\n",
              " ('nutrients', 2),\n",
              " ('nag', 2),\n",
              " ('mockery', 2),\n",
              " ('tantrums', 2),\n",
              " ('rbpdlovedones', 2),\n",
              " ('error', 2),\n",
              " ('filter', 2),\n",
              " ('toddler', 2),\n",
              " ('spine', 2),\n",
              " ('limbs', 2),\n",
              " ('icu', 2),\n",
              " ('facilities', 2),\n",
              " ('smartphone', 2),\n",
              " ('burner', 2),\n",
              " ('mitigate', 2),\n",
              " ('apocalypse', 2),\n",
              " ('autonomy', 2),\n",
              " ('accountability', 2),\n",
              " ('tube', 2),\n",
              " ('shitton', 2),\n",
              " ('inverse', 2),\n",
              " ('nn', 2),\n",
              " ('dependence', 2),\n",
              " ('selfawareness', 2),\n",
              " ('spotlight', 2),\n",
              " ('exploration', 2),\n",
              " ('delve', 2),\n",
              " ('bpds', 2),\n",
              " ('selflessness', 2),\n",
              " ('hype', 2),\n",
              " ('hangups', 2),\n",
              " ('hostage', 2),\n",
              " ('n', 2),\n",
              " ('agoraphobia', 2),\n",
              " ('breakthrough', 2),\n",
              " ('mri', 2),\n",
              " ('advocacy', 2),\n",
              " ('writers', 2),\n",
              " ('musicians', 2),\n",
              " ('cackle', 2),\n",
              " (\"'benjamin\", 2),\n",
              " ('legit', 2),\n",
              " ('maid', 2),\n",
              " ('alberta', 2),\n",
              " ('tottaly', 2),\n",
              " ('cosmiiaaa', 2),\n",
              " ('gunna', 2),\n",
              " ('weisers', 2),\n",
              " ('gulit', 2),\n",
              " ('kinder', 2),\n",
              " ('sail', 2),\n",
              " ('w', 2),\n",
              " ('slam', 2),\n",
              " ('coat', 2),\n",
              " ('clueless', 2),\n",
              " ('peice', 2),\n",
              " ('dianne', 2),\n",
              " ('david', 2),\n",
              " ('disillusion', 2),\n",
              " ('avail', 2),\n",
              " ('fuckem', 2),\n",
              " ('obesity', 2),\n",
              " ('seize', 2),\n",
              " ('prevail', 2),\n",
              " ('mainstream', 2),\n",
              " ('mystery', 2),\n",
              " ('worldview', 2),\n",
              " ('empower', 2),\n",
              " ('do.i', 2),\n",
              " ('sadder', 2),\n",
              " ('asylum', 2),\n",
              " ('platoon', 2),\n",
              " ('ncos', 2),\n",
              " ('woo', 2),\n",
              " ('rotc', 2),\n",
              " ('bct', 2),\n",
              " ('nah', 2),\n",
              " ('webcam', 2),\n",
              " ('fi', 2),\n",
              " ('marvel', 2),\n",
              " ('bass', 2),\n",
              " ('thati', 2),\n",
              " ('will.if', 2),\n",
              " ('rag', 2),\n",
              " ('physiatrist', 2),\n",
              " ('revenue', 2),\n",
              " ('wholl', 2),\n",
              " ('kindred', 2),\n",
              " ('citizens', 2),\n",
              " ('adrenalin', 2),\n",
              " ('sd', 2),\n",
              " ('applaud', 2),\n",
              " ('perspectives', 2),\n",
              " ('selfrealization', 2),\n",
              " ('ruff', 2),\n",
              " ('bot', 2),\n",
              " ('selfimprovement', 2),\n",
              " ('babysteps', 2),\n",
              " ('nikki', 2),\n",
              " ('peruse', 2),\n",
              " ('otherxe2x80x99s', 2),\n",
              " ('locate', 2),\n",
              " ('cham', 2),\n",
              " ('anonimity', 2),\n",
              " ('expound', 2),\n",
              " ('wherewithal', 2),\n",
              " ('sexxo', 2),\n",
              " ('them.and', 2),\n",
              " ('foothold', 2),\n",
              " ('inferiority', 2),\n",
              " ('blare', 2),\n",
              " ('exausted', 2),\n",
              " ('consciously', 2),\n",
              " ('wildlife', 2),\n",
              " ('things.i', 2),\n",
              " ('moron', 2),\n",
              " ('podcast', 2),\n",
              " ('claim.im', 2),\n",
              " ('quetiapine', 2),\n",
              " ('twitchiness', 2),\n",
              " ('childrens', 2),\n",
              " ('neighbourhood', 2),\n",
              " ('her..', 2),\n",
              " ('efficient', 2),\n",
              " ('babes', 2),\n",
              " ('adjustments', 2),\n",
              " ('referrals', 2),\n",
              " ('killers', 2),\n",
              " ('dangers', 2),\n",
              " ('ace', 2),\n",
              " (\"'following\", 2),\n",
              " ('allen', 2),\n",
              " ('procrastination', 2),\n",
              " ('barley', 2),\n",
              " (\"'friend\", 2),\n",
              " ('necklace', 2),\n",
              " ('havehad', 2),\n",
              " ('contest', 2),\n",
              " ('juggle', 2),\n",
              " ('that.and', 2),\n",
              " ('proposal', 2),\n",
              " ('leech', 2),\n",
              " ('ritalin', 2),\n",
              " ('criticisms', 2),\n",
              " ('imho', 2),\n",
              " (\"'seeing\", 2),\n",
              " ('divine', 2),\n",
              " ('birthright', 2),\n",
              " ('ladder', 2),\n",
              " ('quo', 2),\n",
              " ('carrot', 2),\n",
              " ('distinguish', 2),\n",
              " ('tutorials', 2),\n",
              " ('relevance', 2),\n",
              " ('fluid', 2),\n",
              " ('immortality', 2),\n",
              " ('md', 2),\n",
              " ('qualification', 2),\n",
              " ('commenters', 2),\n",
              " (\"'break\", 2),\n",
              " ('divide', 2),\n",
              " ('down.i', 2),\n",
              " ('courtesy', 2),\n",
              " ('interfere', 2),\n",
              " ('vastness', 2),\n",
              " ('niche', 2),\n",
              " ('lion', 2),\n",
              " ('shawshenk', 2),\n",
              " ('providers', 2),\n",
              " ('housemate', 2),\n",
              " ('javascript', 2),\n",
              " ('emo', 2),\n",
              " ('scince', 2),\n",
              " ('practicly', 2),\n",
              " ('heartless', 2),\n",
              " ('guardians', 2),\n",
              " ('oak', 2),\n",
              " ('funeral', 2),\n",
              " ('readjust', 2),\n",
              " ('resolution', 2),\n",
              " ('characterise', 2),\n",
              " (\"'talking\", 2),\n",
              " ('slope', 2),\n",
              " ('amass', 2),\n",
              " ('termination', 2),\n",
              " (\"'giving\", 2),\n",
              " ('meat', 2),\n",
              " ('cantdont', 2),\n",
              " ('cheque', 2),\n",
              " ('impulse', 2),\n",
              " ('aggression', 2),\n",
              " ('mammal', 2),\n",
              " ('yourself..', 2),\n",
              " ('symbols', 2),\n",
              " ('them..', 2),\n",
              " ('clout', 2),\n",
              " ('corpse', 2),\n",
              " ('distortion', 2),\n",
              " ('detriment', 2),\n",
              " ('accommodate', 2),\n",
              " ('expansion', 2),\n",
              " ('carl', 2),\n",
              " ('jung', 2),\n",
              " ('volumes', 2),\n",
              " ('adviser', 2),\n",
              " ('aerodynamics', 2),\n",
              " ('rifle', 2),\n",
              " ('squish', 2),\n",
              " ('celebrities', 2),\n",
              " ('hack', 2),\n",
              " ('necessity', 2),\n",
              " ('hollywood', 2),\n",
              " ('.02', 2),\n",
              " ('persuasion', 2),\n",
              " ('hera', 2),\n",
              " ('multitude', 2),\n",
              " ('whisper', 2),\n",
              " ('anne', 2),\n",
              " ('puppies', 2),\n",
              " ('that.if', 2),\n",
              " ('disabilities', 2),\n",
              " ('ineptitude', 2),\n",
              " ('glimpse', 2),\n",
              " ('that.do', 2),\n",
              " ('anhedonia', 2),\n",
              " ('day.also', 2),\n",
              " ('hope.i', 2),\n",
              " ('honeymoon', 2),\n",
              " ('yak', 2),\n",
              " ('herd', 2),\n",
              " ('cock', 2),\n",
              " ('epic', 2),\n",
              " ('this.you', 2),\n",
              " (\"'damn\", 2),\n",
              " ('carolina', 2),\n",
              " ('admission', 2),\n",
              " ('hostel', 2),\n",
              " (\"'reading\", 2),\n",
              " ('dimension', 2),\n",
              " ('grapple', 2),\n",
              " ('emt', 2),\n",
              " (\"'relationships\", 2),\n",
              " (\"'god\", 2),\n",
              " ('workmans', 2),\n",
              " ('comp', 2),\n",
              " ('dishearten', 2),\n",
              " ('deposit', 2),\n",
              " ('invalidate', 2),\n",
              " ('me..', 2),\n",
              " ('autopilot', 2),\n",
              " ('april', 2),\n",
              " ('postcard', 2),\n",
              " ('outsiders', 2),\n",
              " ('coverage', 2),\n",
              " ('bdsm', 2),\n",
              " ('blur', 2),\n",
              " ('liquidate', 2),\n",
              " ('gen', 2),\n",
              " ('mn', 2),\n",
              " ('days.do', 2),\n",
              " ('detox', 2),\n",
              " ('confinement', 2),\n",
              " (\"'tried\", 2),\n",
              " ('inherit', 2),\n",
              " ('bedside', 2),\n",
              " ('boners', 2),\n",
              " ('polite', 2),\n",
              " ('gaba', 2),\n",
              " ('submissions', 2),\n",
              " ('provoke', 2),\n",
              " ('willl', 2),\n",
              " ('simplicity', 2),\n",
              " ('schooler', 2),\n",
              " ('password', 2),\n",
              " ('villages', 2),\n",
              " ('towns', 2),\n",
              " ('landlord', 2),\n",
              " ('tales', 2),\n",
              " ('selfhelp', 2),\n",
              " ('payback', 2),\n",
              " ('narcissist', 2),\n",
              " ('lecturer', 2),\n",
              " ('myth', 2),\n",
              " ('priest', 2),\n",
              " ('peg', 2),\n",
              " ('burger', 2),\n",
              " ('contradictory', 2),\n",
              " ('climate', 2),\n",
              " ('reluctance', 2),\n",
              " ('vouch', 2),\n",
              " ('christian.i', 2),\n",
              " ('christianity', 2),\n",
              " ('collectors', 2),\n",
              " ('zoo', 2),\n",
              " ('helper', 2),\n",
              " ('clients', 2),\n",
              " ('bf3', 2),\n",
              " ('coupon', 2),\n",
              " ('groceries', 2),\n",
              " ('prophesy', 2),\n",
              " ('bible', 2),\n",
              " ('needy', 2),\n",
              " ('volleyball', 2),\n",
              " ('donald', 2),\n",
              " ('trump', 2),\n",
              " ('bankrupt', 2),\n",
              " ('com', 2),\n",
              " ('dismay', 2),\n",
              " ('exceed', 2),\n",
              " ('hover', 2),\n",
              " ('dart', 2),\n",
              " ('reprieve', 2),\n",
              " ('arlington', 2),\n",
              " ('cory', 2),\n",
              " ('pavement', 2),\n",
              " ('kickstart', 2),\n",
              " ('npr', 2),\n",
              " ('telephone', 2),\n",
              " ('encapsulate', 2),\n",
              " ('vehicle', 2),\n",
              " ('mushroom', 2),\n",
              " ('plethora', 2),\n",
              " ('outreach', 2),\n",
              " ('anything.i', 2),\n",
              " ('school.its', 2),\n",
              " ('entheogens', 2),\n",
              " ('for.i', 2),\n",
              " ('hobby.i', 2),\n",
              " ('induce', 2),\n",
              " ('bakery', 2),\n",
              " ('swank', 2),\n",
              " ('laboratory', 2),\n",
              " ('bleach', 2),\n",
              " ('harbor', 2),\n",
              " ('ancestors', 2),\n",
              " ('rsuicidewatchrsuicidewatch', 2),\n",
              " ('farmers', 2),\n",
              " ('harris', 2),\n",
              " ('howd', 2),\n",
              " ('anew', 2),\n",
              " ('extract', 2),\n",
              " ('victory', 2),\n",
              " ('armor', 2),\n",
              " ('mire', 2),\n",
              " ('be.anyway', 2),\n",
              " ('spurt', 2),\n",
              " ('off.i', 2),\n",
              " ('spirituality', 2),\n",
              " ('remarry', 2),\n",
              " ('rmensrights', 2),\n",
              " ('flash', 2),\n",
              " ('indoors', 2),\n",
              " ('gtim', 2),\n",
              " ('consumer', 2),\n",
              " ('msn', 2),\n",
              " ('sam', 2),\n",
              " ('lifestyles', 2),\n",
              " ('firefly', 2),\n",
              " ('beard', 2),\n",
              " ('museums', 2),\n",
              " ('journals', 2),\n",
              " ('sore', 2),\n",
              " ('pole', 2),\n",
              " ('canyon', 2),\n",
              " ('tour', 2),\n",
              " ('keg', 2),\n",
              " ('cheese', 2),\n",
              " ('mastery', 2),\n",
              " ('trophies', 2),\n",
              " ('cabinet', 2),\n",
              " ('them.but', 2),\n",
              " ('applicants', 2),\n",
              " ('defeatist', 2),\n",
              " ('monologue', 2),\n",
              " ('principles', 2),\n",
              " ('bipolarism', 2),\n",
              " ('dryer', 2),\n",
              " ('sheepskin', 2),\n",
              " ('texture', 2),\n",
              " (\"'killing\", 2),\n",
              " ('stint', 2),\n",
              " ('halfsister', 2),\n",
              " ('guitarist', 2),\n",
              " ('rhythm', 2),\n",
              " ('bubble', 2),\n",
              " ('thing.edit', 2),\n",
              " ('witch', 2),\n",
              " ('livelihood', 2),\n",
              " ('reset', 2),\n",
              " ('disinterested', 2),\n",
              " ('anatomy', 2),\n",
              " ('neurons', 2),\n",
              " ('calendar', 2),\n",
              " ('village', 2),\n",
              " ('ferrari', 2),\n",
              " ('headline', 2),\n",
              " ('boulder', 2),\n",
              " ('that.you', 2),\n",
              " ('sick', 2),\n",
              " ('hitchhikers', 2),\n",
              " ('all.it', 2),\n",
              " ('franklin', 2),\n",
              " ('obligations', 2),\n",
              " ('punk', 2),\n",
              " ('homecoming', 2),\n",
              " ('pond', 2),\n",
              " ('motivations', 2),\n",
              " ('comprehension', 2),\n",
              " ('jiu', 2),\n",
              " ('president', 2),\n",
              " ('therapist.you', 2),\n",
              " ('yourself.so', 2),\n",
              " ('magazine', 2),\n",
              " ('rationalize', 2),\n",
              " ('problemsdisabilities', 2),\n",
              " ('emulate', 2),\n",
              " ('leash', 2),\n",
              " ('deceive', 2),\n",
              " ('posts..', 2),\n",
              " ('overreacting..', 2),\n",
              " ('stepmom', 2),\n",
              " ('want.i', 2),\n",
              " ('masturbation', 2),\n",
              " ('verb', 2),\n",
              " ('elijah', 2),\n",
              " ('too..', 2),\n",
              " ('landscape', 2),\n",
              " ('utter', 2),\n",
              " ('help..', 2),\n",
              " ('implement', 2),\n",
              " ('farmer', 2),\n",
              " ('drivers', 2),\n",
              " ('bitchy', 2),\n",
              " ('ol', 2),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx1xoFV4SJOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d90141f3-73ed-40a2-8c73-4c39a5a2b106"
      },
      "source": [
        "dataset = list(zip(tokens,labels))\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4NEjwpap0wp"
      },
      "source": [
        "# dataset에 있는 모든 단어 집합\n",
        "totalnouns = list(set(sum(list( x[0] for x in dataset ),[])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9pSYasJS2Ce"
      },
      "source": [
        "label_score = {\n",
        "        \"Supportive\" :4,\n",
        "        \"Behavior\" : 1,\n",
        "        \"Ideation\": 2,\n",
        "        \"Indicator\": 3,\n",
        "        \"Attempt\"  : 0,\n",
        "        \"Unknown\" : -1\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxA-NIo1p8L4"
      },
      "source": [
        "dataset = list( (x[0],label_score[x[1]]) for x in dataset )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEmo0ABXTAB9"
      },
      "source": [
        "def make_vocab_dict(tnouns) :\n",
        "  idx = 0\n",
        "  word_idx_dict = {}\n",
        "  idx_word_dict = {}\n",
        "\n",
        "  for nouns in tnouns :\n",
        "    word_idx_dict[nouns] = idx\n",
        "    idx_word_dict[idx] = nouns\n",
        "    idx += 1\n",
        "\n",
        "  return word_idx_dict,idx_word_dict\n",
        "\n",
        "wi,iw = make_vocab_dict(totalnouns)\n",
        "total_noun_cnt = len(wi.items())\n",
        "\n",
        "def encode_post(post,total_noun_cnt,wi) :\n",
        "  bow = [ 0 for _ in range(total_noun_cnt) ]\n",
        "\n",
        "  for w in post :\n",
        "    idx = wi[w]\n",
        "    bow[idx] +=  1\n",
        "\n",
        "  return bow\n",
        "\n",
        "def encode_all_post(dataset,total_noun_cnt,wi) :\n",
        "\n",
        "  bows = []\n",
        "\n",
        "  for post,label in dataset :\n",
        "    bow = encode_post(post,total_noun_cnt,wi)\n",
        "    bows.append(bow)\n",
        "\n",
        "  return bows\n",
        "\n",
        "\n",
        "bows = encode_all_post(dataset,total_noun_cnt,wi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_ur-gaW-ls"
      },
      "source": [
        "def count_word_for_post(post) :\n",
        "  d = {}\n",
        "  for w in post :\n",
        "    try :\n",
        "      d[w] += 1\n",
        "    except :\n",
        "      d[w] = 1\n",
        "\n",
        "  return d\n",
        "\n",
        "def make_df(bows,word_count) :\n",
        "  df = {}\n",
        "\n",
        "  for i in range(word_count) : \n",
        "    df[i] = 0\n",
        "\n",
        "    for d in bows :\n",
        "      if d[i] > 0 :\n",
        "        df[i] += 1\n",
        "\n",
        "  return df\n",
        "\n",
        "from math import log\n",
        "\n",
        "def tf_idf(bows,word_count,df) :\n",
        "  nd = len(bows)\n",
        "  \n",
        "  idf_scores = [ [0 for _ in range(word_count)] for x in range(len(bows)) ]\n",
        "  doc_idx = 0\n",
        "\n",
        "  for d in bows :\n",
        "    tcnt = sum(d)\n",
        "    \n",
        "    for word_idx in range(word_count) :\n",
        "      mother = 1 + df[word_idx]  \n",
        "      son = nd\n",
        "      idf_scores[doc_idx][word_idx] = (log(son/mother)+1) * (d[word_idx]/tcnt)\n",
        "       \n",
        "    doc_idx += 1\n",
        "     \n",
        "  return idf_scores\n",
        "\n",
        "df = make_df(bows,total_noun_cnt)\n",
        "tf_idf_array = tf_idf(bows,total_noun_cnt,df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUUfHsn4f-H2"
      },
      "source": [
        "class LinearModel :\n",
        "    def __init__(self) :\n",
        "        pass\n",
        "\n",
        "    def initW(self,pcodes) :\n",
        "        a = []\n",
        "        wa = []\n",
        "        tw = np.sum(pcodes)\n",
        "\n",
        "        return np.ones(len(pcodes[0])) / tw\n",
        "       \n",
        "\n",
        "    def initB(self,pcodes) :\n",
        "        return -2\n",
        "\n",
        "    def gradfn(self,xp,s) :\n",
        "        return -self.X[xp] * 0.000001 * s     #가중치 행렬에 대한 학습률\n",
        "\n",
        "    def _train(self,xp) :\n",
        "        loss = self.L[xp]\n",
        "\n",
        "        gd = self.gradfn(xp,loss)\n",
        "\n",
        "        self.w += gd\n",
        "        self.b -= loss * 0.000001    #bias에 대한 학습률\n",
        "       # print(self.L[xp])\n",
        "\n",
        "    def train(self) :\n",
        "        for i in range(len(self.X)) :\n",
        "            self._train(i)\n",
        "            print(np.sum(self.L))\n",
        "\n",
        "    def run(self,x,y,di,dd,mc,epoch) :\n",
        "        X = np.array(x)\n",
        "        Y = y\n",
        "\n",
        "        self.X = X\n",
        "\n",
        "        self.w = self.initW(X)\n",
        "        self.Y = Y  \n",
        "        self.b = self.initB(X)\n",
        "\n",
        "        for _ in range(epoch) :\n",
        "            self.P = np.matmul(X,self.w.T) + self.b  \n",
        "            self.L = self.P - self.Y  \n",
        "\n",
        "            self.train()\n",
        "        \n",
        "        return self.w,self.b\n",
        "\n",
        "    def predict(self,x,dd,mc)  :\n",
        "        X = np.array(x)\n",
        "\n",
        "        P = np.matmul(X,self.w.T) + self.b\n",
        "        predicts = []\n",
        "\n",
        "        for i in range(len(X)) :\n",
        "            p = min(int(P[i]+0.5),4)\n",
        "            predicts.append(p)\n",
        "\n",
        "        return predicts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZhzEMqm2LKh"
      },
      "source": [
        "# 일단 제외\n",
        "model = LinearModel()\n",
        "\n",
        "train_x = list( list(y*60  for y in x) for x in tf_idf_array[:350] )\n",
        "train_y = list( x[1] for x in dataset[:350] )\n",
        "\n",
        "test_x = list( list(y*60  for y in x) for x in tf_idf_array[350:] )\n",
        "test_y = list( x[1] for x in dataset[350:] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_e3KAV1dH2U"
      },
      "source": [
        "\n",
        "model = LinearModel()\n",
        "\n",
        "train_x = list( list(y*60  for y in x) for x in tf_idf_array[:200] )\n",
        "train_y = list( x[1] for x in dataset[:200] )\n",
        "\n",
        "test_x = list( list(y*60  for y in x) for x in tf_idf_array[200:350] )\n",
        "test_y = list( x[1] for x in dataset[200:350] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OP6Gql1kjsn"
      },
      "source": [
        "# 모델 피팅\n",
        "model.run(train_x,train_y,wi,iw,total_noun_cnt,epoch=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9vRLY4Gw4Uz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "238863be-76d2-491f-cc2f-2f6b1fd65153"
      },
      "source": [
        "# y 값 예측\n",
        "pred_y = model.predict(test_x,iw,total_noun_cnt)\n",
        "print(pred_y)\n",
        "import pandas as pd \n",
        "\n",
        "pd.DataFrame(pred_y).to_csv(\"2015318434.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 1, 1, 2, 2, 2, 3, 2, 2, 1, 1, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 2, 1, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 0, 2, 1, 2, 2, 2, 1, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 1, 3, 2, 2, 3, 2, 1, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 3, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_lVZrQcdMz7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c98e2573-79ad-46e9-c3ff-8f413277292b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "def get_clf_eval(test,pred):\n",
        "  confusion = confusion_matrix(test,pred)\n",
        "  accuracy = accuracy_score(test,pred)\n",
        "  print(confusion)\n",
        "  print('정확도: {0:.4f}'.format(accuracy))\n",
        "\n",
        "get_clf_eval(test_y,pred_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  2  6  2  0]\n",
            " [ 0  7 15  1  0]\n",
            " [ 0  5 43 10  0]\n",
            " [ 0  1 23  5  0]\n",
            " [ 1  3 13 13  0]]\n",
            "정확도: 0.3667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}